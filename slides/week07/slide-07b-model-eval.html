<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Evaluating Regression Models</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dr. Mine Dogucu" />
    <script src="slide-07b-model-eval_files/header-attrs-2.11/header-attrs.js"></script>
    <link rel="stylesheet" href="slide-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">


class: title-slide







&lt;br&gt;
&lt;br&gt;
.right-panel[ 

# Evaluating Regression Models
## Dr. Mine Dogucu
Examples from [bayesrulesbook.com](https://bayesrulesbook.com)

]

---

class: middle

1. __How fair is the model?__ How was the data collected? By whom and for what purpose? How might the results of the analysis, or the data collection itself, impact individuals and society? What biases or power structures might be baked into this analysis?   
  
--

2. __How wrong is the model?__  George Box famously said: “All models are wrong, but some are useful.” What’s important to know then is, how wrong is our model? Are our model assumptions reasonable?

--

3. __How accurate are the posterior predictive models?__    

---

class: middle

## Checking Model Assumptions


`$$Y_i | \beta_0, \beta_1, \sigma \stackrel{ind}{\sim} N(\mu_i, \sigma^2) \;\; \text{ with } \;\; \mu_i = \beta_0 + \beta_1 X_i  .$$`

1. Conditioned on `\(X\)`, the observed __data__ `\(Y_i\)` on case `\(i\)` is _independent_ of the observed data on any other case `\(j\)`.
2. The typical `\(Y\)` outcome can be written as a _linear function_ of `\(X\)`, `\(\mu = \beta_0 + \beta_1 X\)`.
3. At any `\(X\)` value, `\(Y\)` __varies normally__ around `\(\mu\)` with consistent variability `\(\sigma\)`. 


---

class: middle

## Independence


_When taken alone_, ridership `\(Y\)` is likely correlated over time -- today's ridership likely tells us something about tomorrow's ridership.
Yet much of this correlation, or dependence, can be explained by the time of year and features associated with the time of year -- like temperature `\(X\)`.
Thus, knowing the _temperature_ on two subsequent days may very well "cancel out" the time correlation in their ridership data.

---

class: middle

## Linearity and Constant Variance

&lt;img src="slide-07b-model-eval_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

The relationship between ridership and temperature does appear to be linear. Further, with the slight exception of colder days on which ridership is uniformly small, the variability in ridership does appear to be roughly consistent across the range of temperatures `\(X\)`.

---

class: middle


__Posterior predictive check__

Consider a regression model with response variable `\(Y\)`, predictor `\(X\)`, and a set of regression parameters `\(\theta\)`. For example, in the model above `\(\theta = (\beta_0,\beta_1,\sigma)\)`.  Further, let `\(\left\lbrace \theta^{(1)}, \theta^{(2)}, \ldots, \theta^{(N)}\right\rbrace\)` be an `\(N\)`-length Markov chain for the posterior model of `\(\theta\)`.
Then a "good" Bayesian model will produce _predictions_ of `\(Y\)` with features similar to the _original_ `\(Y\)` data.  To evaluate whether your model satisfies this goal:

1. At each set of posterior plausible parameters `\(\theta^{(i)}\)`, simulate a sample of `\(Y\)` values from the likelihood model, one corresponding to each `\(X\)` in the original sample of size `\(n\)`.  This produces `\(N\)` separate samples of size `\(n\)`.
2. Compare the features of the `\(N\)` simulated `\(Y\)` samples, or a subset of these samples, to those of the original `\(Y\)` data.

---

class: middle


```r
bike_model &lt;- stan_glm(rides ~ temp_feel, data = bikes,
                       family = gaussian,
                       prior_intercept = normal(5000, 1000),
                       prior = normal(100, 40), 
                       prior_aux = exponential(0.0008),
                       chains = 4, iter = 5000*2, seed = 84735, refresh = FALSE)
bike_model_df &lt;- as.data.frame(bike_model)
set.seed(84735)
predict_75 &lt;- bike_model_df %&gt;% 
  mutate(mu = `(Intercept)` + temp_feel*75) %&gt;% 
  mutate(y_new = rnorm(20000, mu, sigma))
```

---

class: middle


```r
first_set &lt;- head(bike_model_df, 1)
beta_0 &lt;- first_set$`(Intercept)`
beta_1 &lt;- first_set$temp_feel
sigma  &lt;- first_set$sigma
set.seed(84735)
one_simulation &lt;- bikes %&gt;% 
  mutate(mu = beta_0 + beta_1 * temp_feel,
         simulated_rides = rnorm(500, mean = mu, sd = sigma)) %&gt;% 
  select(temp_feel, rides, simulated_rides)

head(one_simulation, 2)
```

```
##   temp_feel rides simulated_rides
## 1  64.72625   654        3932.404
## 2  49.04645  1229        1503.185
```

---

class: middle

&lt;img src="slide-07b-model-eval_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

One posterior simulated dataset of ridership (light blue) along with the actual observed ridership data (dark blue)

---

class: middle


```r
# Examine 50 of the 20000 simulated samples
pp_check(bike_model, nreps = 50) + 
  xlab("rides")
```

&lt;img src="slide-07b-model-eval_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;



class: middle

&lt;img src="slide-07b-model-eval_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;


---



class: middle

__observed value__: `\(Y\)`  
__posterior predictive median__: `\(Y'\)`  
__predictive error__: `\(Y - Y'\)`

---

class: middle



```r
predict_75 %&gt;% 
  summarize(mean = mean(y_new), error = 6228 - mean(y_new))
```

```
##       mean    error
## 1 3966.865 2261.135
```

---

class: middle


```r
predict_75 %&gt;% 
  summarize(sd = sd(y_new), error = 6228 - mean(y_new),
            error_scaled = error / sd(y_new))
```

```
##         sd    error error_scaled
## 1 1279.789 2261.135     1.766803
```

---

class: middle


```r
predict_75 %&gt;% 
  summarize(lower_95 = quantile(y_new, 0.025),
            lower_50 = quantile(y_new, 0.25),
            upper_50 = quantile(y_new, 0.75),
            upper_95 = quantile(y_new, 0.975))
```

```
##   lower_95 lower_50 upper_50 upper_95
## 1 1499.832 3096.397 4831.226 6482.125
```

---

class: middle


```r
set.seed(84735)
predictions &lt;- posterior_predict(bike_model, newdata = bikes)

dim(predictions)
```

```
## [1] 20000   500
```

---

class: middle


```r
ppc_intervals(bikes$rides, yrep = predictions, x = bikes$temp_feel, 
              prob = 0.5, prob_outer = 0.95)
```


```r
set.seed(7)
index &lt;- sample(1:500, size = 25)
bikes_small &lt;- bikes[index,]
predictions_small &lt;- predictions[,index]
g1 &lt;- ppc_intervals(bikes$rides, 
  yrep = predictions, 
  x = bikes$temp_feel, 
  prob = 0.5, prob_outer = 0.95)
g2 &lt;- ppc_intervals(bikes_small$rides, 
  yrep = predictions_small, 
  x = bikes_small$temp_feel, 
  prob = 0.5, prob_outer = 0.95)
ggpubr::ggarrange(g1,g2,ncol=2, common.legend = TRUE, legend = "right")
```

&lt;img src="slide-07b-model-eval_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;



















---

class: middle


Let `\(Y_1, Y_2, \ldots, Y_n\)` denote `\(n\)` _observed_ outcomes.  Then each `\(Y_i\)` has a corresponding posterior predictive model with _median_ `\(Y_i'\)` and _median absolute deviation_ `\(\text{mad}_i\)`.  We can evaluate the overall posterior predictive model quality by the following measures:

- The __median absolute error__  `mae`

    `$$\text{mae} = \text{median}_{i \in \{1,2,\ldots,n\}} |Y_i - Y_i'|$$`

- The __scaled median absolute error__ `scaled_mae`

    `$$\text{mae scaled} = \text{median}_{i \in \{1,2,\ldots,n\}} \frac{|Y_i - Y_i'|}{\text{sd}_i}$$`

- `within_50` and `within_95` measure the proportion of observed values `\(Y_i\)` that fall within their 50% and 95% posterior prediction intervals respectively.  


---

class: middle

 

```r
# Posterior predictive summaries
prediction_summary(bike_model, data = bikes)
```

```
##       mae mae_scaled within_50 within_95
## 1 981.277  0.7664511     0.432     0.968
```

---

class: middle


__The k-fold cross validation algorithm__

1. __Create folds.__    
    Let `\(k\)` be some integer from 2 to our original sample size `\(n\)`. Split the data into `\(k\)` __folds__, or subsets, of roughly equal size.    
    
2. __Train and test the model.__    
    - _Train_ the model using the first `\(k - 1\)` data folds combined.
    - _Test_ this model on the `\(k\)`th data fold.
    - Measure the prediction quality (eg: by MAE).
    
3. __Repeat.__    
    Repeat step 2 `\(k - 1\)` times, each time leaving out a different fold for testing.
    
4. __Calculate cross-validation estimates.__    
    Steps 2 and 3 produce `\(k\)` different training models and `\(k\)` corresponding measures of prediction quality. _Average_ these `\(k\)` measures to obtain a single cross-validation estimate of prediction quality.
    
---

class: middle


```r
set.seed(84735)
cv_procedure &lt;- prediction_summary_cv(
  data = bikes, model = bike_model, k = 10)
```

---

class: middle


```r
cv_procedure$folds
```

```
##    fold       mae mae_scaled within_50 within_95
## 1     1  990.1998  0.7699098      0.46      0.98
## 2     2  963.7588  0.7423091      0.40      1.00
## 3     3  951.2922  0.7299569      0.42      0.98
## 4     4 1018.5916  0.7909998      0.46      0.98
## 5     5 1161.5245  0.9091055      0.36      0.96
## 6     6  937.6392  0.7326643      0.46      0.94
## 7     7 1270.8992  1.0061093      0.32      0.96
## 8     8 1111.9120  0.8604959      0.36      1.00
## 9     9 1098.7223  0.8679061      0.40      0.92
## 10   10  786.8053  0.6059727      0.56      0.96
```

---

class: middle



```r
cv_procedure$cv
```

```
##        mae mae_scaled within_50 within_95
## 1 1029.134  0.8015429      0.42     0.968
```


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "pygments",
"highlightLines": true,
"highlightLanguage": "r"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
