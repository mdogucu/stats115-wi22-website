---
title: "Simple Normal Regression"
author: "Dr. Mine Dogucu"
output: 
  xaringan::moon_reader:
    css: ["slide-style.css"]
    seal: false
    nature:
      ratio: 16:9
      highlightStyle: "pygments"
      highlightLines: true
      highlightLanguage: "r"
---

class: title-slide

```{r child = "../setup.Rmd"}
```

```{r echo=FALSE, message=FALSE}
library(bayesrules)
library(tidyverse)
library(rstanarm)
library(bayesplot)
library(janitor)
library(rstan)
```

<br>
<br>
.right-panel[ 

# `r rmarkdown::metadata$title`
## `r rmarkdown::metadata$author`
Examples from [bayesrulesbook.com](https://bayesrulesbook.com)

]

---

```{r}
glimpse(bikes)
```

---

## Rides

.pull-left[


```{r echo = FALSE, fig.height=5}
ggplot(data.frame(x = c(-4,4)), aes(x=x)) + 
  stat_function(fun = dnorm) + 
  labs(y = expression(paste("f(y|",mu,",",sigma,")")), x = "y (rides)") + 
  scale_x_continuous(breaks = c(-3,0,3), labels = c(expression(paste(mu,"- 3 / ",sigma)), expression(mu), expression(paste(mu,"+ 3 / ",sigma))))
```

]

.pull-right[
\begin{split}
Y_i | \mu, \sigma & \stackrel{ind}{\sim} N(\mu, \sigma^2)\\
\mu               & \sim N(\theta, \tau^2)   \\
\sigma            & \sim \text{ some prior model.} \\
\end{split}

]
---

class: middle 

## Regression Model

$Y_i$ the number of rides  
$X_i$ temperature (in Fahrenheit) on day $i$. 

--

$\mu_i = \beta_0 + \beta_1X_i$

--

$\beta_0:$ the typical ridership on days in which the temperature was 0 degrees ( $X_i$=0). It is not interpretable in this case.

$\beta_1:$ the typical change in ridership for every one unit increase in temperature.

---

### Normal likelihood model

\begin{split}
Y_i | \beta_0, \beta_1, \sigma & \stackrel{ind}{\sim} N\left(\mu_i, \sigma^2\right) \;\; \text{ with } \;\; \mu_i = \beta_0 + \beta_1X_i \; .\\
\end{split}

```{r ch-9-normal-assumptions, echo = FALSE, fig.width = 8, fig.height = 4, message=FALSE}
set.seed(454)
x <- rnorm(100, mean = 68, sd = 12)
y_1 <- -2511 + 88*x + rnorm(100, mean=0, sd = 2000)
y_2 <- -2511 + 88*x + rnorm(100, mean=0, sd = 200)
bikes_sim <- data.frame(x, y_1, y_2) %>% filter(y_1 > 0)
g1 <- ggplot(bikes_sim, aes(x=x,y=y_1)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) + 
    #scale_x_continuous(breaks = c(25)) + 
    #scale_y_continuous(breaks = c(0,30), limits = c(min(y_1,y_2),max(y_1,y_2))) +
    lims(y = c(min(y_1,y_2),max(y_1,y_2))) + 
    labs(x = "x (temp)", y = "y (rides)")
g2 <- ggplot(bikes_sim, aes(x=x,y=y_2)) + 
    geom_point() + 
    geom_smooth(method = "lm", se = FALSE) + 
    #scale_x_continuous(breaks = c(25)) + 
    #scale_y_continuous(breaks = c(30), limits = c(min(y_1,y_2),max(y_1,y_2))) + 
    lims(y = c(min(y_1,y_2),max(y_1,y_2))) + 
    labs(x = "x (temp)", y = "y (rides)")
    
gridExtra::grid.arrange(g1,g2,ncol=2)
  
```

These simulations show two cases where $\beta_0 = -2000$ and slope $\beta_1 = 100$.
On the left $\sigma = 2000$ and on the right $\sigma = 200$ (right). In both cases, the model line is defined by $\beta_0 + \beta_1 x = -2000 + 100 x$.


---

## Prior Models

$\text{likelihood model:} \; \; \; Y_i | \beta_0, \beta_1, \sigma \;\;\;\stackrel{ind}{\sim} N\left(\mu_i, \sigma^2\right)\text{ with } \mu_i = \beta_0 + \beta_1X_i$

$\text{prior models:}$ 

$\beta_0\sim N(m_0, s_0^2 )$  
$\beta_1\sim N(m_1, s_1^2 )$  
$\sigma \sim \text{Exp}(l)$

--

Recall: 

$\text{Exp}(l) = \text{Gamma}(1, l)$

---

class: middle

## Prior Models

$$\begin{split}
Y_i | \beta_0, \beta_1, \sigma & \stackrel{ind}{\sim} N\left(\mu_i, \sigma^2\right) \;\; \text{ with } \;\; \mu_i = \beta_0 + \beta_1X_i \\
\beta_0  & \sim N\left(3482, 3937^2 \right)  \\
\beta_1  & \sim N\left(0, 351^2 \right) \\
\sigma   & \sim \text{Exp}(0.00064) \; .\\
\end{split}$$


---

class: middle

```{r fig.height=5}
plot_normal(mean = 5000, sd = 1000) + 
  labs(x = "beta_0c", y = "pdf")
```

---

class: middle


```{r fig.height=5}
plot_normal(mean = 100, sd = 40) + 
  labs(x = "beta_1", y = "pdf")
```

---

class: middle


```{r fig.height=5}
plot_gamma(shape = 1, rate = 0.0008) + 
  labs(x = "sigma", y = "pdf")
```

---


class: middle

$$\begin{split}
Y_i | \beta_0, \beta_1, \sigma & \stackrel{ind}{\sim} N\left(\mu_i, \sigma^2\right) \;\; \text{ with } \;\; \mu_i = \beta_0 + \beta_1X_i \\
\beta_{0c}  & \sim N\left(5000, 1000^2 \right)  \\
\beta_1  & \sim N\left(100, 40^2 \right) \\
\sigma   & \sim \text{Exp}(0.0008)  .\\
\end{split}$$

---

class: middle 

```{r echo = FALSE, warning=FALSE, fig.height=6, fig.width=15}
g1 <- plot_normal(mean = 5000, sd = 1000) + 
  labs(x = "beta_0c", y = "pdf")
g2 <- plot_normal(mean = 100, sd = 40) + 
  labs(x = "beta_1", y = "pdf")
g3 <- plot_gamma(shape = 1, rate = 0.0008) + 
  labs(x = "sigma", y = "pdf") + 
  lims(x = c(0,7500))
gridExtra::grid.arrange(g1,g2,g3,ncol=3)
```

---

class: middle

## Simulation via `rstanarm`

```{r cache=TRUE}
bike_model <- stan_glm(rides ~ temp_feel, data = bikes,
                       family = gaussian,
                       prior_intercept = normal(5000, 1000),
                       prior = normal(100, 40), 
                       prior_aux = exponential(0.0008),
                       chains = 4, iter = 5000*2, seed = 84735,
                       refresh = FALSE) 
```

The `refresh = FALSE` prevents printing out your chains and iterations, especially useful in R Markdown.

---

class: middle

```{r}
# Effective sample size ratio and Rhat
neff_ratio(bike_model)

rhat(bike_model)

```

The effective sample size ratios are slightly above 1 and the R-hat values are very close to 1, indicating that the chains are stable, mixing quickly, and behaving much like an independent sample.

---

class: middle

```{r fig.width=12}
mcmc_trace(bike_model, size = 0.1)
```

---

class: middle


```{r fig.width=12, fig.height=6}
mcmc_dens_overlay(bike_model)
```

---

```{r}
# STEP 1: DEFINE the model
stan_bike_model <- "
  data {
    int<lower = 0> n;
    vector[n] Y;
    vector[n] X;
  }
  parameters {
    real beta0;
    real beta1;
    real<lower = 0> sigma;
  }
  model {
    Y ~ normal(beta0 + beta1 * X, sigma);
    beta0 ~ normal(-2000, 1000);
    beta1 ~ normal(100, 40);
    sigma ~ exponential(0.0008);
  }
"
```

---

```{r}
# STEP 2: SIMULATE the posterior
stan_bike_sim <- 
  stan(model_code = stan_bike_model, 
       data = list(n = nrow(bikes), Y = bikes$rides, X = bikes$temp_feel), 
       chains = 4, iter = 5000*2, seed = 84735)
```

---

```{r}
broom.mixed::tidy(bike_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)
```


```{r echo = FALSE}
model_summary <- tidy(bike_model, effects = c("fixed", "aux"),
                      conf.int = TRUE, conf.level = 0.80)
b0_median <- model_summary[1,2]
b1_median <- model_summary[2,2]
b1_lower <- model_summary[2,4]
b1_upper <- model_summary[2,5]
```

Referring to the `tidy()` summary, the __posterior median relationship__ is

$$\begin{equation}
`r round(b0_median,2)` + `r round(b1_median,2)` X .
(\#eq:post-med-trend-ch9)
\end{equation}$$

---

class: middle

```{r}
# Store the 4 chains for each parameter in 1 data frame
bike_model_df <- as.data.frame(bike_model)
# Check it out
nrow(bike_model_df)
head(bike_model_df, 3)
```

---

class: middle

```{r}
# 50 simulated model lines
bikes %>%
  tidybayes::add_fitted_draws(bike_model, n = 50) %>%
  ggplot(aes(x = temp_feel, y = rides)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15) + 
    geom_point(data = bikes, size = 0.05)
```


---


class: middle 

```{r}
# Tabulate the beta_1 values that exceed 0
bike_model_df %>% 
  mutate(exceeds_0 = temp_feel > 0) %>% 
  tabyl(exceeds_0)
```


---

class: middle

## Posterior Prediction

Suppose a weather report indicates that tomorrow will be a 75-degree day in D.C. What's your posterior guess of the number of riders that Capital Bikeshare should anticipate?


---

class: middle

```{r echo = FALSE}
pred <- round(b0_median,2) + (round(b1_median,2)*75)
```


Your natural first crack at this question might be to plug the 75-degree temperature into the posterior median model.
Thus, we expect that there will be `r round(pred)` riders tomorrow:

$$`r round(b0_median,2)` + `r round(b1_median,2)`*75 = `r pred` .$$

--

Not quiet.

---

class: middle

Recall that this singular prediction ignores two potential sources of variability:

- __Sampling variability__ in the data    
    The observed ridership outcomes, $Y$, typically _deviate_ from the model line. That is, we don't expect every 75-degree day to have the same exact number of rides.
    
- __Posterior variability__ in parameters $(\beta_0, \beta_1, \sigma)$    

The posterior median model is merely the center in a _range_ of plausible model lines $\beta_0 + \beta_1 X$. We should consider this entire range as well as that in $\sigma$, the degree to which observations might deviate from the model lines.

The __posterior predictive model__ of a new data point $Y_{\text{new}}$ accounts for both sources of variability.


---

Now, we don't actually have a nice, tidy formula for the posterior pdf of our regression parameters, $f(\beta_0,\beta_1,\sigma|\vec{y})$, and thus can't get a nice tidy formula for the posterior predictive pdf $f\left(y_{\text{new}} | \vec{y}\right)$.
What we _do_ have is 20,000 sets of parameters in the Markov chain $\left(\beta_0^{(i)},\beta_1^{(i)},\sigma^{(i)}\right)$.
We can then _approximate_ the posterior predictive model for $Y_{\text{new}}$ at $X = 75$ by simulating a ridership prediction from the Normal model evaluated each parameter set:

$$Y_{\text{new}}^{(i)} | \beta_0, \beta_1, \sigma  \; \sim \; N\left(\mu^{(i)}, \left(\sigma^{(i)}\right)^2\right) \;\; \text{ with } \;\; \mu^{(i)} = \beta_0^{(i)} + \beta_1^{(i)} \cdot 75.$$

Thus, each of the 20,000 parameter sets in our Markov chain (left) produces a unique prediction (right):

$$\left[
\begin{array}{lll} 
\beta_0^{(1)} & \beta_1^{(1)} & \sigma^{(1)} \\
\beta_0^{(2)} & \beta_1^{(2)} & \sigma^{(2)} \\
\vdots & \vdots & \vdots \\
\beta_0^{(20000)} & \beta_1^{(20000)} & \sigma^{(20000)} \\
\end{array}
\right]
\;\; \longrightarrow \;\;
\left[
\begin{array}{l} 
Y_{\text{new}}^{(1)} \\
Y_{\text{new}}^{(2)} \\
\vdots \\
Y_{\text{new}}^{(20000)} \\
\end{array}
\right]$$

The resulting collection of 20,000 predictions, $\left\lbrace Y_{\text{new}}^{(1)}, Y_{\text{new}}^{(2)}, \ldots, Y_{\text{new}}^{(20000)} \right\rbrace$, _approximates_ the posterior predictive model of ridership $Y$ on 75-degree days.\index{posterior predictive model}
We will obtain this approximation both "by hand," which helps us build some powerful intuition, and using shortcut R functions.


---

```{r}
first_set <- head(bike_model_df, 1)
first_set
```


---

```{r}
mu <- first_set$`(Intercept)` + first_set$temp_feel * 75
mu
```


---

```{r}
set.seed(84735)
y_new <- rnorm(1, mean = mu, sd = first_set$sigma)
y_new
```

---

```{r}
# Predict rides for each parameter set in the chain
set.seed(84735)
predict_75 <- bike_model_df %>% 
  mutate(mu = `(Intercept)` + temp_feel*75,
         y_new = rnorm(20000, mean = mu, sd = sigma))
```


```{r}
head(predict_75, 3)

```

---

```{r}
# Construct 80% posterior credible intervals
predict_75 %>% 
  summarize(lower_mu = quantile(mu, 0.025),
            upper_mu = quantile(mu, 0.975),
            lower_new = quantile(y_new, 0.025),
            upper_new = quantile(y_new, 0.975))
```


---

.pull-left[
```{r eval = FALSE}
# Plot the posterior model of the typical ridership on 75 degree days
ggplot(predict_75, aes(x = mu)) + 
  geom_density()
# Plot the posterior predictive model of tomorrow's ridership
ggplot(predict_75, aes(x = y_new)) + 
  geom_density()
```

]



```{r ch9-post-pred, fig.width = 4, echo = FALSE}
g1 <- ggplot(predict_75, aes(x = mu)) + 
  geom_density() +
  lims(x = range(predict_75$y_new), y = c(0, 0.0065))
g2 <- ggplot(predict_75, aes(x = y_new)) + 
  geom_density() +
  lims(x = range(predict_75$y_new), y = c(0, 0.0065))
grid.arrange(g1,g2,ncol=2)
```

---

## Posterior Prediction with rstanarm

```{r}
# Simulate a set of predictions
set.seed(84735)
shortcut_prediction <- 
  posterior_predict(bike_model, newdata = data.frame(temp_feel = 75))
```

```{r}
head(shortcut_prediction, 3)
```



---

```{r}
# Construct a 95% posterior credible interval
posterior_interval(shortcut_prediction, prob = 0.95)

# Plot the approximate predictive model
mcmc_dens(shortcut_prediction) + 
  xlab("predicted ridership on a 75 degree day")
```

